{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# MLOps Hackathon\n",
    "\n",
    "This hackathon is based on the [open-source Turbo Template](https://github.com/teamdatatonic/vertex-pipelines-end-to-end-samples).\n",
    "Through this notebook series you'll get hands-on with the template and Google Cloud.\n",
    "The hackathon is structured into the following exercises:\n",
    "\n",
    "1. [Health check](./01_health_check.ipynb)\n",
    "1. **[Run pipelines](./02_run_pipelines.ipynb) - this notebook**\n",
    "1. [Promote model](./03_promote_model.ipynb)\n",
    "1. [Challenge: Model monitoring](./04_monitoring_challenge.ipynb)\n",
    "1. [Challenge: Real-time predictions](./05_realtime_challenge.ipynb)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook you'll run your first training and prediction pipelines. \n",
    "You'll learn about containers, pipelines, caching, and `make` as a productivity accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate\n",
    "\n",
    "Set your project ID and authenticate using your Google Account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [core/account].\n",
      "\n",
      "You are running on a Google Compute Engine virtual machine.\n",
      "It is recommended that you use service accounts for authentication.\n",
      "\n",
      "You can run:\n",
      "\n",
      "  $ gcloud config set account `ACCOUNT`\n",
      "\n",
      "to switch accounts if necessary.\n",
      "\n",
      "Your credentials may be visible to others with access to this\n",
      "virtual machine. Are you sure you want to authenticate with\n",
      "your personal account?\n",
      "\n",
      "Do you want to continue (Y/n)?  "
     ]
    }
   ],
   "source": [
    "VERTEX_PROJECT_ID = \"dt-sky-mlops-hackathon-dev\"\n",
    "GOOGLE_ACCOUNT = \"elliott.murray@sky.uk\"\n",
    "! gcloud config set project {VERTEX_PROJECT_ID}\n",
    "! gcloud config set account {GOOGLE_ACCOUNT}\n",
    "! gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running any pipelines, make sure you have created a `env.sh` and replaced the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training Pipeline\n",
    "\n",
    "Vertex AI Pipelines uses KubeFlow to orchestrate your training steps, as such you'll need to:\n",
    "\n",
    "1. Compile the pipeline\n",
    "1. Build dependent Docker containers\n",
    "1. Run the pipeline in Vertex AI\n",
    "\n",
    "The already templated training pipeline will execute a pipeline similar to the image below in Vertex AI:\n",
    "\n",
    "![Training Pipeline](../docs/images/training_pipeline.png)\n",
    "\n",
    "Don't worry about executing steps 1-3 manually (and each time you run your pipeline!), simply run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! make training wait=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the pipeline executes, here's a more detailed explanation of what's happening:\n",
    "\n",
    "**1. Compile the pipeline:** By using the KubeFlow SDK, you've compiled the training pipeline in `pipelines/training` to YAML.\n",
    "\n",
    "**2. Build dependent Docker containers:** In `model` you can maintain your training (and prediction) code which is containerised and pushed to [Artifact Registry](https://cloud.google.com/artifact-registry). \n",
    "In this way, your training pipeline can execute your training code in the `Train model` pipeline step.\n",
    "\n",
    "**3. Run the pipeline in Vertex AI:** By using the Vertex AI Python SDK and the pipeline YAML file, you execute your training pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚û°Ô∏è Exercise:** With the help of `make help`, try to execute the steps 1-3 individually.\n",
    "\n",
    "**‚û°Ô∏è Exercise:** Have you noticed the caching option? What happens if you run the pipeline with cachine enabled vs. disabled. Why is one option preferred over the other?\n",
    "\n",
    "**‚û°Ô∏è Exercise:** Locate the model training code. Then:\n",
    "1. Update the training code (e.g. add a logging command).\n",
    "2. Rebuild the container using `make`\n",
    "3. Run the training pipeline with caching enabled. What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Prediction Pipeline\n",
    "\n",
    "After running a successful training pipeline job, run the prediction pipeline which will look similar to:\n",
    "\n",
    "<img src=\"../docs/images/prediction_pipeline.png\" alt=\"image\" width=\"500\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! make prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Note:** The command has the following true/false flags:\n",
    "\n",
    "- `build` - re-build containers for training & prediction code (limit by setting `images=training` to build only one of the containers)\n",
    "- `compile` - re-compile the pipeline to YAML\n",
    "- `wait` - run the pipeline (a-)sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚û°Ô∏è Exercise:** Can you locate the batch prediction job in Vertex AI? What are the inputs and outputs of the job?\n",
    "\n",
    "## Monitoring\n",
    "\n",
    "**‚û°Ô∏è Exercise:** Do you notice any monitoring as part of the batch prediction job? What is monitored? Are the any alerts?\n",
    "\n",
    "**‚û°Ô∏è Exercise:** Where in the code can you configure to receive email notifications in case of any alerts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check your understanding\n",
    "\n",
    "**‚û°Ô∏è Exercise:** Understanding of containers:\n",
    "- Why do we need 2x containers?\n",
    "- When are they built?\n",
    "- In which cases do you need to rebuild a container, when can you skip it?\n",
    "- Can you locate the containers in the Google Cloud console?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "You've successfully run your first training and prediction pipeline in Vertex AI! üéâ Now you're ready for the next exercise!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb5c7b0035bb37e2e2e56e6840dfdd8f7fa070884ae8e041fbcae450545b1006"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
